{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5d87b42201e74ac320bc00dce267d44f5f134edfec9046f67f672f289707ff6a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Group Chat Analysis System\n",
    "Merupakan suatu sistem untuk menganalisa suatu pesan di dalam group apakah termasuk pesan yang baik, buruk, benar, atau hoax. Dengan analisa tersebut, nantinya dapat disimpulkan bahwa semakin tinggi dan banyak pesan yang teranalisa buruk atau hoax maka group tersebut dapat dipastikan adalah group yang tidak benar. Dan sebaliknya, jika di dalam grup tersebut banyak pesan baik dan informasi yang disampaikan adalah benar, maka dapat disimpulkan bahwa group tersebut adalah group yang baik.\n",
    "\n",
    "Sistem ini disusun oleh\n",
    "\n",
    "- Fitri Amalia Langgundi\n",
    "- Khoerul Umam\n",
    "- Muhammad Ikhsan Hadi\n",
    "- Rozak Ilham Aditya\n",
    "\n",
    "Dalam mata kuliah : Sistem Pakar"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Group Chat Analysis \n",
    "Inisalisasi library yang dibutuhkan. Beberapa library yang digunakan diantaranya, library untuk memfilter pesan, library untuk mengolah link, library untuk mengolah dataset, library untuk menentukan tingkat karakteristik pesan, dan library lain akan ditambahkan seiring dengan bertambahnya kompleksitas sistem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import string\n",
    "import tldextract\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from difflib import SequenceMatcher as sm"
   ]
  },
  {
   "source": [
    "#### Inisialisai Class\n",
    "Class yang digunakan diberi nama class Pakar. Di dalamnya terdapat berbagai fungsi yang akan memproses data pesan sehingga dapat diketahui karakteristik pesan tersebut. Saat ini sistem hanya dapat memproses satu per satu pesan."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pakar:\n",
    "    def __init__ (self):\n",
    "        self.text = ''\n",
    "        self.text_cl = ''\n",
    "        self.link = []\n",
    "        self.domaindataset = pd.read_csv('domain.csv', skiprows=0)\n",
    "        self.badurl = pd.read_csv('urldata.csv', skiprows=0)\n",
    "        self.result_hoax_arr = []\n",
    "        self.result_kata_arr = []\n",
    "        self.result_hoax = 0\n",
    "        self.result_kata = 0\n",
    "        self.mean_kotor = []\n",
    "        self.mean_hoax_cumulative = []\n",
    "        self.mean_kotor_cumulative = []\n",
    "\n",
    "    def process(self, text):\n",
    "        self.text = text\n",
    "        text_cl = re.sub(r'https?:\\/\\/\\S*', \"\", text)\n",
    "        self.text_cl = self.stem_text(text_cl);\n",
    "        self.checkLink()\n",
    "        self.checkText(True)\n",
    "        self.checkText(False)\n",
    "\n",
    "    def stem_text(self, text):\n",
    "        factory = StemmerFactory()\n",
    "        stemmer = factory.create_stemmer() \n",
    "        textToClean = text\n",
    "        cleanText = stemmer.stem(textToClean) \n",
    "        cleanText = cleanText.lower()\n",
    "        cleanText = re.sub(r\"\\d+\", \"\", cleanText)\n",
    "        cleanText = cleanText.translate(str.maketrans('','', string.punctuation))\n",
    "        cleanText = cleanText.strip()\n",
    "        tokens = nltk.tokenize.word_tokenize(text)\n",
    "        kataSambung = set(stopwords.words('indonesian'))\n",
    "        cleanTokens = []\n",
    "        for te in tokens:\n",
    "            if te not in kataSambung:\n",
    "                cleanTokens.append(te)\n",
    "        result = ' '\n",
    "        return result.join(cleanTokens)\n",
    "\n",
    "    def checkLink(self):\n",
    "        regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
    "        url = re.findall(regex, self.text)       \n",
    "        for x in url:\n",
    "            link = x[0]\n",
    "            domain = tldextract.extract(x[0])\n",
    "            suffix = domain.suffix\n",
    "\n",
    "            if (domain.suffix == ''):\n",
    "                suffix = domain.domain\n",
    "            \n",
    "            checkDomain = self.domaindataset.loc[self.domaindataset['Domain'] == '.' + suffix]\n",
    "            cleanUrl = re.sub(r'https?:\\/\\/', \"\", x[0])\n",
    "            checkUrl = self.badurl.loc[self.badurl['url'] == cleanUrl]\n",
    "\n",
    "            if checkUrl.empty == False:\n",
    "                self.link.append([link, checkUrl['label'].values, checkDomain['Domain'].values, checkDomain['Type'].values, checkDomain['Sponsoring Organisation'].values])\n",
    "                break\n",
    "                \n",
    "            if (link.find('https') != -1):\n",
    "                if (checkDomain.empty == False):\n",
    "                    self.link.append([link, 'Aman', checkDomain['Domain'].values, checkDomain['Type'].values, checkDomain['Sponsoring Organisation'].values])\n",
    "                else:\n",
    "                    self.link.append([link, 'Mencurigakan', checkDomain['Domain'].values, checkDomain['Type'].values, checkDomain['Sponsoring Organisation'].values])\n",
    "            else:\n",
    "                if (checkDomain.empty == False):\n",
    "                    self.link.append([link, 'Keamanan Lemah', checkDomain['Domain'].values, checkDomain['Type'].values, checkDomain['Sponsoring Organisation'].values])\n",
    "                else:\n",
    "                    self.link.append([link, 'Bahaya', checkDomain['Domain'].values, checkDomain['Type'].values, checkDomain['Sponsoring Organisation'].values])\n",
    "\n",
    "    def checkText(self, hoax = True):\n",
    "        mean_kotor = []\n",
    "        mean_hoax = []\n",
    "        if hoax:\n",
    "            dataset = pd.read_csv('D:\\Project\\Self\\Pakar\\hoax.csv', header=None)\n",
    "            for index, row in dataset.iterrows():\n",
    "                mean_hoax.append(sm(None, self.text_cl, row[0]).ratio())\n",
    "            self.result_hoax = np.amax(np.array(mean_hoax))\n",
    "            self.result_hoax_arr = mean_hoax\n",
    "            self.mean_hoax_cumulative.append(mean_hoax)\n",
    "        else:\n",
    "            dataset = pd.read_csv('D:\\Project\\Self\\Pakar\\katakotor.csv', header=None)\n",
    "            for word in self.text_cl.split():\n",
    "                for index, row in dataset.iterrows():\n",
    "                    mean_kotor.append(sm(None, word, row[0]).ratio())\n",
    "            self.result_kata = np.amax(np.array(mean_kotor))\n",
    "            self.result_kata_arr = mean_kotor\n",
    "            self.mean_kotor_cumulative.append(mean_kotor)\n",
    "    \n",
    "    def remove_emoji(self, text):\n",
    "        regrex_pattern = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "        return regrex_pattern.sub(r'',text)\n",
    "\n",
    "    def get_domain_dt(self):\n",
    "        return self.domaindataset\n",
    "    \n",
    "    def get_result(self):\n",
    "        return self.result\n",
    "\n",
    "    def get_text(self):\n",
    "        print(\"Original : \\n\")\n",
    "        print(self.text)\n",
    "        print(\"\\n\\nCleaned : \\n\")\n",
    "        print(self.text_cl)\n",
    "\n",
    "    def get_data(self, data = ''):\n",
    "        if data == 'hoax':\n",
    "            return self.result_hoax_arr\n",
    "        else:\n",
    "            return self.result_kata_arr\n",
    "\n",
    "    def print_plot(self, plot = ''):\n",
    "        if plot == 'hoax':\n",
    "            pt.plot(self.result_hoax_arr)\n",
    "        else:\n",
    "            pt.plot(self.result_kata_arr)\n",
    "\n",
    "    def show_result(self):\n",
    "        print('Tingkat ke-hoax-an pesan: ', round(self.result_hoax, 3))\n",
    "        print('Tingkat ke-kotoran-an pesan: ', round(self.result_kata, 3), '\\n')\n",
    "        if (self.link != None):\n",
    "            for x in self.link:\n",
    "                print('Link: ', x[0], ' terindikasi ', x[1])\n",
    "                print('Domain: ', x[2])\n",
    "                print('Tipe Link: ', x[3])\n",
    "                print('Keterangan: ', x[4], '\\n')\n",
    "\n",
    "    def get_all_results(self, plot = ''):\n",
    "        if plot == 'hoax':\n",
    "            pt.plot(self.mean_hoax_cumulative)\n",
    "        else:\n",
    "            pt.plot(self.mean_kotor_cumulative)\n",
    "\n",
    "\n"
   ]
  },
  {
   "source": [
    "### Testing\n",
    "Testing digunakan per pesan. Dilakukan seperti di bawah ini, pesan akan diketahui seberapa dekat karakteristik dengan dataset yang digunakan, termasuk jika ada link di dalamnya, akan di ketahui link mana yang berbahaya dan link mana yang aman."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Pakar()\n",
    "t.process('Bagi yang di sekitar Unnes, khususnya jln taman siswa. Berita kehilangan Dompet warna pink Di dlmnya ada KTP, NPWP,  uang tunai, ATM a.n Yulia Anjani.Brgkli ada yg menemukan, bs hub 089 683 743 789 Mohon bantuannya, Akan di beri imbalan bagi yg menemukan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t.show_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.print_plot('hoax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hoaxs = pd.DataFrame(np.array(t.get_data('hoax')))\n",
    "hoaxs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-4a9a306fca84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mcleanEmoji\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhoaxDetection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove_emoji\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcleanEmoji\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m''\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcleanEmoji\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcleanEmoji\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mhoaxDetection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanEmoji\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mhoaxDetection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_all_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-bd92785f25c0>\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckLink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheckText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstem_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-bd92785f25c0>\u001b[0m in \u001b[0;36mcheckText\u001b[1;34m(self, hoax)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mmean_kotor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_kata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_kotor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult_kata_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_kotor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean_kotor_cumulative\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_kotor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2666\u001b[0m     \"\"\"\n\u001b[0;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[1;32m-> 2668\u001b[1;33m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[0;32m   2669\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "hoaxDetection = Pakar()\n",
    "datasetChat = pd.read_csv('D:\\Project\\Self\\Pakar\\datachat.csv', header=None, skiprows=1)\n",
    "cleanDataChat = []\n",
    "for index, row in datasetChat.iterrows():\n",
    "    cleanEmoji = hoaxDetection.remove_emoji(row[0])\n",
    "    if cleanEmoji != '' and cleanEmoji != ' ' and cleanEmoji is not None:\n",
    "        hoaxDetection.process(cleanEmoji)\n",
    "hoaxDetection.get_all_result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}